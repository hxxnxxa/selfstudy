<div align='center'>
    <h1>Character decomposition to resolve class imbalance problem in Hangul OCR</h1>
</div>

<h2>Author</h2>
<li>현대자동차</li> 
<li>김건욱, 손재민, 이강휴, 민재식</li>
<li>Accepted 12 Aug 2022</li>


<h2>Abstract</h2>
<li>한글 OCR에서 클래스 불균형 문제를 해결하기 위한 문자 분해</li>
<li>한글 OCR(Optical Character Recognition)에 대한 새로운 접근 방식 제시</li>
<li>한글은 11,172개의 서로 다른 문자를 52개 자소로 표현할 수 있는 글자로 각 문자를 자소의 조합으로 표현할 수 있다.</li>
<li>전체 문자 수가 신경망의 용량을 압도할 수 있기 때문에 기존 OCR 인코딩 방식은 자주 사용하는 더 작은 문자 집합을 미리 정의한다.</li>
<li><b>자소 인코딩(grapheme encoding)</b>이 한글 OCR에 효율적일 뿐만 아니라 성능도 있음을 보여준다.</li>
<li>여기서 제시하는 네트워크는 한글 OCR의 두 가지 주요 문제인 클래스 불균형과 대상 클래스 선택을 해결한다는 것을 보여준다.</li>

<h2>1. Introduction</h2>

<b>1) 문제점 제시</b> 
<li>영어와 달리 각 한글 문자는 [자음 + 모음], [자음 + 모음 + 자음] 과 같이 2개 또는 3개의 자소 조합으로 표현된다. </li>
<li>총 문자 수는 11,172자에 달하며 이는 표현 능력에서 소프트맥스 계층을 쉽게 고갈시킬 수 있다. </li>
<li>따라서 현재 한글 OCR 시스템은 문자의 일부만 활용하고 가장 많이 사용되는 1000자 정도를 유지하고 있다.</li>

<br>

<b>2) 이전 연구의 한계 :</b> 
<li>이전 접근 방식은 균일하게 분포된 문자를 제공하기 위해 가상 이미지를 생성하려고 시도했지만 대상 도메인을 얻는 데 중요하지 않을 수 있는 문자를 학습하기 위한 추가 학습 시간이 발생한다.</li>

<br>

<b>3) 해결책 제시</b>
<li>이 문제를 해결하기 위해 문자를 자소로 분해할 것을 제안한다.</li>
<li>한글은 자음과 모음이 위치마다 정해진 규칙(첫 자음+모음 또는 첫자음+모음+마지막 자음)으로 존재하므로 인식 대상을 문자에서 자소로 변경하면 11,172개 클래스를 52개 클래스로 분류하는 문제를 대체할 수 있다.</li>
<li>이 분해에는 두 가지 주요 이점이 있다. 첫 번째, 모델이 문자의 다수 클래스와 소수 클래스의 자소를 동일한 자소 클래스로 분류하는 방법을 배우게 함으로써 클래스 불균형 문제가 크게 완화된다.</li>
<li>두 번째, 기존에 학습한 자음과 모음을 조합하여 학습하지 않은 문자도 인식할 수 있어 한글 전체의 문자 인식이 가능하다.</li>
<li>한국어 텍스트 인식 작업에 대한 벤치마크를 구축하고 제안한 방법의 효과를 실험적으로 검증하였다.</li>


<h2>2. Related Work</h2>

<b>1) 기존 사용 사례</b>
<li>번호판 인식</li> 
<li>필기체 인식</li>
<li>신용 카드 번호 인식</li>

<br>

<b>2) 기존 인식 작업</b> 
<li>시퀀스 예측으로 모델링했다.</li> 
<li>시각적 특징은 ResNet과 같은 CNN backbone에서 추출되고 LSTM 및 GRU와 같은 RNN 계층에 공급되어 연결주의적 시간분류(CTC)로 훈련된다.</li>

<br>

<b>3) 최근 접근 방식</b> 
<li>자연어 처리에서 개발된 Transformer의 Attention 매커니즘에서 영감을 받아 Image patch 간의 상호 작용을 모델링하는 ViT(Vision Transformer)와 같은 보다 강력한 backbone을 사용한다.</li>

<br>

<b>4) 기존 연구 한계</b>
<li>자주 사용하는 문자만 선택하여 학습 : 선택한 집합 외부의 문자를 절대 인식할 수 없다.</li>
<li>11,172자 모두 학습 : 실제 상황에서 거의 나타나지 않는 문자를 분류하기 위해 더 넓은 범위의 문자를 포함하는 많은 데이터가 필요하다.</li>
<li>학습된 모델을 다른 도메인 간에 전송할 수 있는지 여부는 불분명하다.</li>

<h2>3. Method</h2>
<li>한글 장면 텍스트 인식의 경우 입력은 이미지이고 출력은 한글 텍스트이다.</li>
<li>인코더-디코더 구조를 사용했다.</li>
<li>인코더 : CNN 레이어 φ 와 Transformer Unit ψ 은 문자를 정의하는 자소 간의 Context를 학습하는데 사용된다.</li>
<li>디코더 : 주어진 Feature Map의 각 자소에 해당하는 Feature에 속하는 방법을 학습한다.</li>
<li>자소가 존재하지 않는 예외적인 유형의 문자가 있는데 이를 해결하기 위해 "무성음"이라는 가상 클래스를 추가했다.</li>

<h2>4. Experiments</h2>
<h3>4.1 Datasets</h3>

<b>Benchmark dataset :</b> https://github.com/mandal4/HangulNet

<br>

<b>1) AI Hub</b>
<li>100,000개의 한글 이미지로 구성되어있다.</li>
<li>문자 검출을 제외한 문자 인식 성능을 평가하기 위해 총 674,110개의 텍스트 영역을 추출하였다.</li>
<li>이 중 10,000개는 테스트 세트로 분리되며 나머지는 훈련 데이터로 사용된다.</li>

<br>

<b>2) MLT-h</b>
<li>다국어 텍스트 감지 및 스크립트 식별 문제를 해결하기 위해 ICDAR에서 강력한 읽기 대회의 일부로 도입되었다.</li>
<li>평가를 위해 MLT17 테스트 세트에서 한글 텍스트 영역만 이용하고 이름을 MLT-h로 지정한다.</li>
<li>이 데이터셋에서 많은 주석 오류를 발견했기 때문에 노이즈가 많은 레이블을 수정했다.</li>

<br>

<b>3) Standard Foreign Words(SFW)</b>
<li>해당 논문에서 제안한 모델이 한글 문자 인식에서 클래스 불균형 문제를 효과적으로 완화하는지 객관적으로 평가하기 위해 SynthTiger를 사용하여 다수의 소수 클래스를 포함하는 새로운 데이터셋을 합성했다.</li>
<li>데이터셋은 국립국어원에 등록된 총 18,831개의 표준어를 포함하고 있으며 이는 테스트세트로만 사용된다.</li>

<br>

<b>4) Unseen Characters</b>
<li>SFW 데이터셋에서 일반적인 문자 인코딩으로 표현할 수 없는 72개의 문자를 선택하여 문자당 이미지를 생성했다.</li>
<li>그 다음 이 데이터셋에서 다양한 모델의 성능을 측정하여 보이지 않는 문자의 견고성을 비교한다.</li>

<h3>4.2 Implementation Details</h3>
<li>인코더 : 5개의 Transformer 레이어를 사용한다.</li>
<li>자소 합성은 단순히 UTF-8 유니코드로 구성된다.</li>
<li>학습률은 0.001, 배치크기는 300, 에포크는 800,000회로 모델을 훈련했다.</li>
<li>입력 이미지는 32x128 크기이다.</li>
<li>모든 실험은 두 대의 Nvidia V100에서 수행된다.</li>

<h3>4.3 Experimental Settings</h3>
<li>논문 3페이지 참고</li>

<h3>4.4 Quantitative Results</h3>
<li>논문의 Table 1 참고</li>

<h3>4.5 Further Analysis</h3>
<li>논문의 Figure 4, Figure 5, Figure 6 참고</li>

<h2>5. Conclusion</h3>
<li>본 논문은 인코더-디코더 구조의 자소 단위 한글 텍스트 인식기를 제안한다.</li>
<li>자소 수준 분류 접근 방식은 자소를 분류하는 방법을 확인함으로써 클래스 불균형 문제가 상당히 완화된다.</li>
<li>한글 OCR의 클래스 불균형 및 대상 클래스 선택 문제를 해결하기 위해 새로운 접근법을 제시했다.</li>

<br>

<a href='https://arxiv.org/abs/2208.06079'>논문 원본 보기</a>